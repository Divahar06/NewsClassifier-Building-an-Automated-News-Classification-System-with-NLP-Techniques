{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b04d5fe3",
   "metadata": {},
   "source": [
    "# Problem Statemet for Web Scraping of information :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6b3244",
   "metadata": {},
   "source": [
    "## Web Scraping:- Choose news websites (e.g., BBC, The Hindu, Times Now, CNN) and use web scraping\n",
    " tools or libraries (e.g., BeautifulSoup,Selenium) to extract news articles.- Retrieve the title and content of each news article. Ensure that you have a diverse dataset\n",
    " covering various topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9471eb07",
   "metadata": {},
   "source": [
    "# Indian Express News Scraper\n",
    "\n",
    "This Python script is designed to scrape news articles from the Indian Express website, specifically from various sections like Business, Entertainment, Sports, Politics, Lifestyle, Education, and Technology. The scraped data is then stored in a CSV file.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Make sure you have the following Python libraries installed:\n",
    "\n",
    "- `requests`: Used to make HTTP requests.\n",
    "- `BeautifulSoup`: A library for pulling data out of HTML and XML files.\n",
    "- `csv`: A module for reading and writing CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebc65b0",
   "metadata": {},
   "source": [
    "# Understanding the Code\n",
    "\n",
    "The provided Python script is a web scraping tool tailored for extracting news articles from the Indian Express website. It encompasses several functions and logic for navigating through different sections of the site, scraping article details, and saving the data to a CSV file. Here's a breakdown of the main components:\n",
    "\n",
    "## `scrape_news(url, section_label, num_pages=3)`\n",
    "\n",
    "This function serves as the core of the scraper. It extracts news articles from a specified section of the Indian Express website. The key parameters are:\n",
    "\n",
    "- `url`: The URL of the section to be scraped.\n",
    "- `section_label`: The label or category of the section, e.g., Business, Entertainment, etc.\n",
    "- `num_pages`: The number of pages to scrape for the specified section (default is 3).\n",
    "\n",
    "The function iterates through the specified number of pages, calling other functions to extract article details.\n",
    "\n",
    "## `scrape_technology_page(url)`\n",
    "\n",
    "This function is specifically designed for the Technology section. It extracts article information from each page within the section.\n",
    "\n",
    "## `scrape_page(url, section_label)`\n",
    "\n",
    "A generic function used for non-Technology sections. It extracts article details from each page of the specified section.\n",
    "\n",
    "## `scrape_article(article_url)`\n",
    "\n",
    "Responsible for extracting the title and content of an individual article. It fetches the HTML of the article URL and uses BeautifulSoup to parse and retrieve relevant information.\n",
    "\n",
    "## `scrape_homepage_sections(base_url)`\n",
    "\n",
    "This function iterates through different sections of the Indian Express website, calling `scrape_news` for each section. The sections include Business, Entertainment, Sports, Politics, Lifestyle, Education, and Technology.\n",
    "\n",
    "## `homepage_url`\n",
    "\n",
    "This variable stores the URL of the Indian Express homepage. You can modify this if you want to scrape from a different source.\n",
    "\n",
    "## Writing to CSV\n",
    "\n",
    "The script writes the scraped data to a CSV file named `indian_express_combined_news_1.csv`. The CSV file contains columns for the title, content, and section of each article. You can customize the filename in the script as needed.\n",
    "\n",
    "**Note:** Ensure that you comply with the website's terms of service and legal guidelines when using web scraping tools. Misuse or unauthorized scraping may violate website policies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d59e3dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping section: Business, page: 1\n",
      "Scraping section: Business, page: 2\n",
      "Scraping section: Business, page: 3\n",
      "Scraping section: Business, page: 4\n",
      "Scraping section: Business, page: 5\n",
      "Scraping section: Entertainment, page: 1\n",
      "Scraping section: Entertainment, page: 2\n",
      "Scraping section: Entertainment, page: 3\n",
      "Scraping section: Entertainment, page: 4\n",
      "Scraping section: Entertainment, page: 5\n",
      "Scraping section: Sports, page: 1\n",
      "Scraping section: Sports, page: 2\n",
      "Scraping section: Sports, page: 3\n",
      "Scraping section: Sports, page: 4\n",
      "Scraping section: Sports, page: 5\n",
      "Scraping section: Politics, page: 1\n",
      "Scraping section: Politics, page: 2\n",
      "Scraping section: Politics, page: 3\n",
      "Scraping section: Politics, page: 4\n",
      "Scraping section: Politics, page: 5\n",
      "Scraping section: Lifestyle, page: 1\n",
      "Scraping section: Lifestyle, page: 2\n",
      "Scraping section: Lifestyle, page: 3\n",
      "Scraping section: Lifestyle, page: 4\n",
      "Scraping section: Lifestyle, page: 5\n",
      "Scraping section: Education, page: 1\n",
      "Scraping section: Education, page: 2\n",
      "Scraping section: Education, page: 3\n",
      "Scraping section: Education, page: 4\n",
      "Scraping section: Education, page: 5\n",
      "Scraping section: Technology, page: 1\n",
      "Scraping section: Technology, page: 2\n",
      "Scraping section: Technology, page: 3\n",
      "Scraping section: Technology, page: 4\n",
      "Scraping section: Technology, page: 5\n",
      "Data has been written to indian_express_combined_news_1.csv.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def scrape_news(url, section_label, num_pages=3):\n",
    "    all_data = []\n",
    "\n",
    "    for page_number in range(1, num_pages + 1):\n",
    "        page_url = f'{url}page/{page_number}/'\n",
    "        print(f'Scraping section: {section_label}, page: {page_number}')\n",
    "        if section_label.lower() == 'technology':\n",
    "            page_data = scrape_technology_page(page_url)\n",
    "        else:\n",
    "            page_data = scrape_page(page_url, section_label)\n",
    "        all_data.extend(page_data)\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def scrape_technology_page(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all article links on the page\n",
    "    article_list = soup.find('ul', class_='article-list')\n",
    "    article_items = article_list.find_all('li')\n",
    "\n",
    "    # Initialize a list to store the data\n",
    "    data = []\n",
    "\n",
    "    # Iterate over each article\n",
    "    for item in article_items:\n",
    "        article_url = item.find('h3').find('a')['href']\n",
    "        article_title, article_content = scrape_article(article_url)\n",
    "\n",
    "        # Append the results to the data list\n",
    "        data.append([article_title, article_content, 'Technology'])\n",
    "\n",
    "    return data\n",
    "\n",
    "def scrape_page(url, section_label):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all article links on the page\n",
    "    article_links = soup.select('.img-context h2.title a')\n",
    "\n",
    "    # Initialize a list to store the data\n",
    "    data = []\n",
    "\n",
    "    # Iterate over each article link\n",
    "    for link in article_links:\n",
    "        article_url = link['href']\n",
    "        article_title, article_content = scrape_article(article_url)\n",
    "\n",
    "        # Append the results to the data list\n",
    "        if article_title and article_content:  # Check if title and content are not None\n",
    "            data.append([article_title, article_content, section_label])\n",
    "\n",
    "    return data\n",
    "\n",
    "def scrape_article(article_url):\n",
    "    response = requests.get(article_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract title\n",
    "    title_element = soup.find('h1', itemprop='headline', class_='native_story_title')\n",
    "    title = title_element.text.strip() if title_element else None\n",
    "\n",
    "    # Extract paragraphs\n",
    "    content_element = soup.find('div', class_='full-details')\n",
    "    content = '\\n'.join([paragraph.text.strip() for paragraph in content_element.find_all('p')]) if content_element else None\n",
    "\n",
    "    return title, content\n",
    "\n",
    "def scrape_homepage_sections(base_url):\n",
    "    sections = {\n",
    "        'Business': f'{base_url}section/business/',\n",
    "        'Entertainment': f'{base_url}section/entertainment/',\n",
    "        'Sports': f'{base_url}section/sports/',\n",
    "        'Politics': f'{base_url}section/political-pulse/',\n",
    "        'Lifestyle': f'{base_url}section/lifestyle/',\n",
    "        'Education': f'{base_url}section/education/',\n",
    "        'Technology': f'{base_url}section/technology/',\n",
    "    }\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    for label, url in sections.items():\n",
    "        section_data = scrape_news(url, label, num_pages=5)\n",
    "        all_data.extend(section_data)\n",
    "\n",
    "    return all_data\n",
    "\n",
    "# Indian Express homepage URL\n",
    "homepage_url = 'https://indianexpress.com/'\n",
    "\n",
    "# Scrape news from different sections on the homepage with pagination\n",
    "all_data = scrape_homepage_sections(homepage_url)\n",
    "\n",
    "# Write the data to a CSV file\n",
    "csv_filename = 'indian_express_combined_news_1.csv'\n",
    "header = ['Title', 'Content', 'Section']\n",
    "\n",
    "with open(csv_filename, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    \n",
    "    # Write the header\n",
    "    csv_writer.writerow(header)\n",
    "    \n",
    "    # Write the data\n",
    "    csv_writer.writerows(all_data)\n",
    "\n",
    "print(f'Data has been written to {csv_filename}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff1809b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
